{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-2.4.5.tar.gz (217.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 217.8 MB 1.6 kB/s eta 0:00:01   |                                | 389 kB 235 kB/s eta 0:15:24     |▋                               | 4.1 MB 223 kB/s eta 0:15:58     |█                               | 6.7 MB 786 kB/s eta 0:04:29     |█▍                              | 9.2 MB 1.4 MB/s eta 0:02:25     |██                              | 13.6 MB 6.4 MB/s eta 0:00:32     |██▍                             | 16.1 MB 1.4 MB/s eta 0:02:23     |██▉                             | 19.5 MB 793 kB/s eta 0:04:10     |███                             | 21.1 MB 750 kB/s eta 0:04:23     |███▎                            | 22.4 MB 691 kB/s eta 0:04:43     |████▊                           | 31.9 MB 863 kB/s eta 0:03:36     |█████▉                          | 39.9 MB 1.5 MB/s eta 0:02:00     |██████                          | 40.6 MB 1.4 MB/s eta 0:02:05     |██████                          | 41.6 MB 1.4 MB/s eta 0:02:04     |██████▌                         | 44.5 MB 472 kB/s eta 0:06:07     |███████▌                        | 50.9 MB 202 kB/s eta 0:13:43     |████████▉                       | 59.9 MB 1.3 MB/s eta 0:02:00     |████████▉                       | 60.1 MB 4.7 MB/s eta 0:00:34     |██████████▋                     | 72.6 MB 4.3 MB/s eta 0:00:34     |███████████▏                    | 75.8 MB 1.2 MB/s eta 0:01:58     |███████████▏                    | 76.0 MB 1.2 MB/s eta 0:01:58     |███████████▎                    | 77.0 MB 1.5 MB/s eta 0:01:31     |███████████▍                    | 77.2 MB 1.5 MB/s eta 0:01:31     |███████████▊                    | 80.1 MB 1.2 MB/s eta 0:02:00     |████████████                    | 81.1 MB 1.1 MB/s eta 0:02:03     |████████████▏                   | 82.5 MB 684 kB/s eta 0:03:18     |████████████▏                   | 82.7 MB 684 kB/s eta 0:03:18     |████████████▍                   | 84.2 MB 683 kB/s eta 0:03:16     |█████████████▏                  | 89.8 MB 645 kB/s eta 0:03:19     |█████████████▌                  | 92.2 MB 109 kB/s eta 0:19:13     |█████████████▊                  | 93.6 MB 681 kB/s eta 0:03:03     |██████████████▍                 | 97.9 MB 1.0 MB/s eta 0:01:58     |███████████████▏                | 103.4 MB 723 kB/s eta 0:02:39     |████████████████                | 109.4 MB 1.6 MB/s eta 0:01:08     |█████████████████▌              | 119.3 MB 119 kB/s eta 0:13:43     |███████████████████▍            | 131.9 MB 1.0 MB/s eta 0:01:26     |████████████████████            | 135.5 MB 1.1 MB/s eta 0:01:17     |████████████████████            | 135.7 MB 1.1 MB/s eta 0:01:17     |████████████████████            | 136.0 MB 1.0 MB/s eta 0:01:21     |████████████████████▎           | 138.0 MB 1.3 MB/s eta 0:01:03     |██████████████████████▉         | 155.7 MB 1.2 MB/s eta 0:00:52     |███████████████████████         | 156.0 MB 90 kB/s eta 0:11:21     |█████████████████████████       | 170.2 MB 1.3 MB/s eta 0:00:37     |█████████████████████████       | 170.8 MB 1.3 MB/s eta 0:00:36     |█████████████████████████▍      | 172.7 MB 762 kB/s eta 0:01:00     |█████████████████████████▊      | 175.0 MB 1.1 MB/s eta 0:00:41     |██████████████████████████      | 177.1 MB 4.7 MB/s eta 0:00:09     |██████████████████████████▏     | 178.1 MB 8.3 MB/s eta 0:00:05     |███████████████████████████▍    | 186.3 MB 927 kB/s eta 0:00:34     |████████████████████████████▋   | 194.7 MB 3.4 MB/s eta 0:00:07     |████████████████████████████▋   | 194.8 MB 3.4 MB/s eta 0:00:07     |████████████████████████████▉   | 195.9 MB 1.7 MB/s eta 0:00:14     |█████████████████████████████▌  | 201.1 MB 1.2 MB/s eta 0:00:15     |█████████████████████████████▉  | 203.0 MB 1.0 MB/s eta 0:00:15     |██████████████████████████████▎ | 206.0 MB 654 kB/s eta 0:00:18\n",
      "\u001b[?25hCollecting py4j==0.10.7\n",
      "  Downloading py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 403 kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=84bc4de43eda1aa0b51fbd7bd397d01ee55944a0d7b0ab76b6477fbdd4e584f4\n",
      "  Stored in directory: /home/dmitry/.cache/pip/wheels/01/c0/03/1c241c9c482b647d4d99412a98a5c7f87472728ad41ae55e1e\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Привет, в этой практике мы с вами применим наши знания по PySpark и постараемся изучить что-то новое в процессе выполнения.\n",
    "<br>В занятии используется датасет собранный на основе данных <a href=\"https://www.kaggle.com/chicago/chicago-taxi-rides-2016\">Chicago Taxi Rides 2016</a>\n",
    "<br>Полная <a href=\"https://spark.apache.org/docs/latest/api/python/index.html\">документация PySpark</a>.\n",
    "<br>Схема данны:\n",
    "<br>|-- taxi_id = идентификатор таксиста\n",
    "<br>|-- trip_start_timestamp = время начала поездки\n",
    "<br>|-- trip_end_timestamp = время окончания поездки\n",
    "<br>|-- trip_seconds = время длительности поездки в секундах\n",
    "<br>|-- trip_miles = мили проиденные во время поездки\n",
    "<br>|-- fare = транспортные расходы\n",
    "<br>|-- tips = назначенные чаевые\n",
    "<br>|-- trip_total = общая стоимость поездки\n",
    "<br>|-- payment_type = тип оплаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('PySparkTasks').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.session.timeZone\", \"GMT+3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте <a href=\"https://github.com/AlexKbit/stepik-ds-course/raw/master/Week3/spark-tasks/taxi_data.parquet\">taxi_data.parquet</a> и загрузите используя <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\">SparkAPI</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o28.parquet.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2359)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:594)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:645)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-798271f39d46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'taxi_data.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \"\"\"\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o28.parquet.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:100)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2359)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:594)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:645)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('taxi_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№1 Посчитайте количество загруженных строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Число строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2540712"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим схему данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- taxi_id: integer (nullable = true)\n",
      " |-- trip_start_timestamp: timestamp (nullable = true)\n",
      " |-- trip_end_timestamp: timestamp (nullable = true)\n",
      " |-- trip_seconds: integer (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- fare: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- trip_total: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№2 Чему равна корреляция и ковариация между длиной маршрута и ценой за поездку? Ответ округлите до 5 знаков после запятой.\n",
    "<br>Подробнее <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.corr\">corr</a> & <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.cov\">cov</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44816"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(df.corr('trip_miles', 'trip_total'), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.96914"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(df.cov('trip_miles', 'trip_total'), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№3 Найдите количество, среднее, cреднеквадратическое отклонение, минимум и максимум для длины маршрута и цены за поездку? Ответ округлите до 1 знака после запятой. Подробнее <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe\">describe</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        trip_miles|\n",
      "+-------+------------------+\n",
      "|  count|           2540677|\n",
      "|   mean|3.0005873828090266|\n",
      "| stddev|  5.25716922943536|\n",
      "|    min|               0.0|\n",
      "|    max|             900.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(['trip_miles']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        trip_total|\n",
      "+-------+------------------+\n",
      "|  count|           2540672|\n",
      "|   mean|15.913560215564042|\n",
      "| stddev|30.546699217618237|\n",
      "|    min|               0.0|\n",
      "|    max|           9276.69|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(['trip_total']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№4 Найдите самый НЕ популярный вид оплаты.\n",
    "<br>Подробнее <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy\">groupBy</a> <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy\">orderBy</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|payment_type|my_count|\n",
      "+------------+--------+\n",
      "|    Way2ride|       3|\n",
      "|       Pcard|     878|\n",
      "|      Prcard|     968|\n",
      "|     Dispute|    1842|\n",
      "|     Unknown|    5180|\n",
      "|   No Charge|   12843|\n",
      "| Credit Card| 1108843|\n",
      "|        Cash| 1410155|\n",
      "+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "df.groupBy('payment_type').agg(sf.count('*').alias('my_count')).orderBy('my_count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№5 Найдите идентификатор таксиста выполнившего наибольшее число заказов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|taxi_id|count|\n",
      "+-------+-----+\n",
      "|    316| 2225|\n",
      "|   6591| 2083|\n",
      "|   5071| 2080|\n",
      "|   8740| 2067|\n",
      "|   6008| 2033|\n",
      "|   8629| 2024|\n",
      "|   1462| 2007|\n",
      "|    375| 1986|\n",
      "|   8751| 1938|\n",
      "|   5357| 1930|\n",
      "|   8264| 1909|\n",
      "|   1168| 1809|\n",
      "|   1946| 1803|\n",
      "|    336| 1799|\n",
      "|   1521| 1799|\n",
      "|   3253| 1764|\n",
      "|   8561| 1760|\n",
      "|   8344| 1743|\n",
      "|   8496| 1742|\n",
      "|   6482| 1740|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.groupBy('taxi_id').count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№6 Чему равна средняя цена среди поездок, оплаченных наличными? Ответ округлите до 5 знака.\n",
    "<br> Подробней <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.where\">where</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|round(avg(trip_total), 5)|\n",
      "+-------------------------+\n",
      "|                 12.03526|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.select('payment_type').distinct().show()\n",
    "df.where(df['payment_type'] == 'Cash').groupBy().avg('trip_total').select(sf.round('avg(trip_total)', 5)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№7 Сколько таксистов проехало больше 1000 миль за все время выполнения заказов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "| 2860|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('taxi_id').agg(sf.sum('trip_miles').alias('miles_sum')).where(col('miles_sum') > 1000.0).orderBy(desc('miles_sum')).groupBy().count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№8 Сколько миль проехал пассажир в самой долгой поездке? (Ответ округлите до целого)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-------------------+------------+----------+----+----+----------+------------+\n",
      "|taxi_id|trip_start_timestamp| trip_end_timestamp|trip_seconds|trip_miles|fare|tips|trip_total|payment_type|\n",
      "+-------+--------------------+-------------------+------------+----------+----+----+----------+------------+\n",
      "|   4161| 2016-11-14 16:00:00|2016-11-15 16:00:00|       86399|       0.0|3.25| 0.0|      3.25|        Cash|\n",
      "+-------+--------------------+-------------------+------------+----------+----+----+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(desc('trip_seconds')).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№9 Каков средний заработок всех таксистов? Ответ округлите до 5-ого знака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|round(avg(sum(trip_total)), 5)|\n",
      "+------------------------------+\n",
      "|                    8218.85627|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"taxi_id\").isNotNull()).groupBy('taxi_id').agg(sf.sum('trip_total')).groupBy().avg('sum(trip_total)').select(sf.round('avg(sum(trip_total))', 5)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№10 Сколько поездок начиналось в самый загруженный час?\n",
    "<br>Используйте функцию <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.hour\">hour</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+------+\n",
      "|CAST((hour(trip_start_timestamp) / 6) AS INT)| count|\n",
      "+---------------------------------------------+------+\n",
      "|                                            2|889631|\n",
      "|                                            3|843054|\n",
      "|                                            1|538737|\n",
      "|                                            0|269290|\n",
      "+---------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(hour('trip_start_timestamp')).count().withColumn('divided',(col('hour(trip_start_timestamp)')/6).cast('integer')).orderBy(desc('hour(trip_start_timestamp)')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№11 Сколько поездок началось во второй четверти дня?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+------+\n",
      "|CAST((hour(trip_start_timestamp) / 6) AS INT)| count|\n",
      "+---------------------------------------------+------+\n",
      "|                                            2|889631|\n",
      "|                                            3|843054|\n",
      "|                                            1|538737|\n",
      "|                                            0|269290|\n",
      "+---------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy((hour('trip_start_timestamp')/6).cast('integer')).count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№12 Найдите топ три даты, в которые было суммарно больше всего чаевых? (Чаевые выдаются после совершения поездки)\n",
    "<br> Ожидаемый формат дат YYYY-MM-DD\n",
    "<br>Вам может понадобится конвертация типов <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.cast\">cast</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------+\n",
      "|CAST(trip_end_timestamp AS DATE)|         sum(tips)|\n",
      "+--------------------------------+------------------+\n",
      "|                      2016-11-03|110102.37000000013|\n",
      "|                      2016-11-09|106187.87999999986|\n",
      "|                      2016-11-16| 99993.77000000038|\n",
      "|                      2016-12-01| 97954.88000000014|\n",
      "|                      2016-11-10| 97263.90000000063|\n",
      "|                      2016-11-04| 96244.83000000005|\n",
      "|                      2016-11-17| 95188.28000000052|\n",
      "|                      2016-12-08| 94128.17000000039|\n",
      "|                      2016-11-08| 90394.94000000024|\n",
      "|                      2016-12-15| 89820.06000000052|\n",
      "|                      2016-11-18|  89685.4800000007|\n",
      "|                      2016-11-02| 89175.40000000023|\n",
      "|                      2016-11-30| 88441.87000000021|\n",
      "|                      2016-11-07| 87743.85000000036|\n",
      "|                      2016-11-15| 85591.05999999978|\n",
      "|                      2016-12-14| 85584.15000000011|\n",
      "|                      2016-11-11| 85330.95000000014|\n",
      "|                      2016-12-09| 84563.74999999993|\n",
      "|                      2016-12-07| 83296.74000000034|\n",
      "|                      2016-12-16| 81553.82000000037|\n",
      "+--------------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(col('trip_end_timestamp').cast(DateType())).sum('tips').orderBy(desc('sum(tips)')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№13 Сколько было заказов в дату с наибольшим спросом?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----+\n",
      "|CAST(trip_start_timestamp AS DATE)|count|\n",
      "+----------------------------------+-----+\n",
      "|                        2016-11-03|61259|\n",
      "|                        2016-12-16|59137|\n",
      "|                        2016-12-09|58583|\n",
      "|                        2016-12-15|57084|\n",
      "|                        2016-11-04|56800|\n",
      "|                        2016-12-08|56786|\n",
      "|                        2016-11-18|56382|\n",
      "|                        2016-12-02|55914|\n",
      "|                        2016-12-01|55298|\n",
      "|                        2016-12-14|53096|\n",
      "|                        2016-11-02|51677|\n",
      "|                        2016-11-05|51341|\n",
      "|                        2016-11-10|50970|\n",
      "|                        2016-11-30|50904|\n",
      "|                        2016-11-09|50894|\n",
      "|                        2016-11-17|50849|\n",
      "|                        2016-11-11|50148|\n",
      "|                        2016-12-07|49653|\n",
      "|                        2016-11-16|49653|\n",
      "|                        2016-11-08|48566|\n",
      "+----------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(col('trip_start_timestamp').cast(DateType())).count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузите данные о марках машин из датасета <a href=\"https://github.com/AlexKbit/stepik-ds-course/raw/master/Week3/spark-tasks/taxi_cars_data.parquet\">taxi_cars_data.parquet</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_car = spark.read.parquet('taxi_cars_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|taxi_id|          car_model|\n",
      "+-------+-------------------+\n",
      "|   1159|       Toyota Prius|\n",
      "|   7273|Ford Crown Victoria|\n",
      "|   2904|        Honda Civic|\n",
      "|   3210|        Ford Fusion|\n",
      "|   2088|       Toyota Camry|\n",
      "|   4821|Ford Crown Victoria|\n",
      "|   2069|    Hyundai Elantra|\n",
      "|   6240|       Toyota Camry|\n",
      "|    296|     Hyundai Sonata|\n",
      "|   2136|Ford Crown Victoria|\n",
      "|   1436|     Toyota Corolla|\n",
      "|   1090|     Toyota Corolla|\n",
      "|   7711|        Lincoln MKZ|\n",
      "|   1572|Ford Crown Victoria|\n",
      "|   3959|     Chevrolet Aveo|\n",
      "|   5645|  Chevrolet Lacetti|\n",
      "|   5149|            Audi A7|\n",
      "|   6366|     Hyundai Sonata|\n",
      "|    451|     Hyundai Accent|\n",
      "|   1669|           Kia Ceed|\n",
      "+-------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_car.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№14 Какая марка машины самая распрастранненая среди таксистов?\n",
    "<br>Подробнее <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.split\">split</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+\n",
      "|split(car_model,  )[0]|count|\n",
      "+----------------------+-----+\n",
      "|                  Ford| 1484|\n",
      "|               Hyundai|  792|\n",
      "|                Toyota|  691|\n",
      "|             Chevrolet|  473|\n",
      "|                   Kia|  265|\n",
      "|                  Audi|  250|\n",
      "|               Lincoln|  247|\n",
      "|                 Honda|  246|\n",
      "|            Volkswagen|  244|\n",
      "|                Nissan|  225|\n",
      "+----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_car.groupBy(split(df_car['car_model'], \" \")[0]).count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "№15 Сколько раз и какая модель машин чаще всего встречается в поездках?\n",
    "<br>Подробнее <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join\">join</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|          car_model| count|\n",
      "+-------------------+------+\n",
      "|Ford Crown Victoria|388682|\n",
      "|     Hyundai Accent|150764|\n",
      "|           Kia Ceed|143649|\n",
      "|     Hyundai Sonata|141570|\n",
      "|        Ford Mondeo|135466|\n",
      "|    Hyundai Elantra|134722|\n",
      "|        Honda Civic|133848|\n",
      "|            Audi A7|129168|\n",
      "|  Chevrolet Lacetti|128803|\n",
      "|     Toyota Corolla|125434|\n",
      "|        Lincoln MKZ|124616|\n",
      "|       Toyota Camry|123249|\n",
      "|    Volkswagen Golf|118449|\n",
      "|        Ford Taurus|116836|\n",
      "|        Ford Fusion|113828|\n",
      "|       Toyota Prius|113099|\n",
      "|     Chevrolet Aveo|109326|\n",
      "|      Nissan Altima|108038|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.join(df_car, 'taxi_id').groupBy('car_model').count().orderBy(desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почувствуй силу сжатия! сохрани DataFrame в csv и сравни размеры файлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1).write.csv('out_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь загрузите данные из csv и проверьте типы методом printSchema()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('out_table.csv').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не забудьте посетить SparkUI и изучить историю ваших задач."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
